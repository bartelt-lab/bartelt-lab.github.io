<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CORE at ICLR 2025</title>
    <link rel="stylesheet" href="style.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Fira+Code:wght@300..700&family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&family=Montserrat:ital,wght@0,100..900;1,100..900&family=Nunito+Sans:ital,opsz,wght@0,6..12,200..1000;1,6..12,200..1000&family=Open+Sans:ital,wght@0,300..800;1,300..800&family=PT+Serif:ital,wght@0,400;0,700;1,400;1,700&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Roboto+Mono:ital,wght@0,100..700;1,100..700&family=Source+Code+Pro:ital,wght@0,200..900;1,200..900&display=swap" rel="stylesheet">
</head>
<body>
    <div class="banner">
        <div class="container">
            <div class="banner-text">CORE research group at ICLR and ICSE 2025</div>
        </div>
    </div>
    <div class="container">
        <div class="content">
            <div class="content-text">
                6 papers authored or co-authored by CORE members have been accepted at the main conference and workshops at The 2025 International Conference on Learning Representations (ICLR) in Singapore and The 2025 International Conference on Software Engineering (ICSE) in Seoul.
            </div>
        </div>

        <h2>ICLR Main Conference</h2>

        <a href="https://arxiv.org/abs/2408.08761" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/marton_mitigating.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization</div>
                    <div class="tags">
                        <div class="tag">University of Mannheim</div>
                        <div class="tag">TU Clausthal</div>
                        <div class="tag">University of Rostock</div>
                    </div>
                    <div class="description">
                        The paper introduces SYMPOL, a novel method that integrates symbolic, tree-based models with policy gradient methods to enhance interpretability in reinforcement learning (RL). 
                        By enabling direct, end-to-end optimization of axis-aligned decision trees within standard on-policy RL algorithms, SYMPOL addresses the challenge of information loss associated with traditional neural network policies. 
                    </div>
                </div>
            </div>
        </a>

        <a href="https://arxiv.org/abs/2407.14561" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/ndif.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals</div>
                    <div class="tags">
                        <div class="tag">Northeastern University</div>
                        <div class="tag">TU Clausthal</div>
                        <div class="tag">University of Hamburg</div>
                    </div>
                    <div class="description">
                        The paper introduces NNsight, an open-source extension to PyTorch enabling deferred remote execution, and NDIF, a scalable inference service for sharing GPU resources and pretrained models. 
                        Together, they facilitate transparent access to the internals of large neural networks, such as large language models, without requiring individual hosting of customized models.
                    </div>
                </div>
            </div>
        </a>

        <h2>ICSE Main Conference</h2>

        <a href="https://arxiv.org/abs/2502.21068" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/guide.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">GUIDE: LLM-Driven GUI Generation Decomposition for Automated Prototyping</div>
                    <div class="tags">
                        <div class="tag">TU Clausthal</div>
                        <div class="tag">Karsruhe Institute of Technology</div>
                        <div class="tag">University of Mannheim</div>
                    </div>
                    <div class="description">
                        The paper introduces GUIDE, a novel approach that leverages large language models (LLMs) to automate the generation of graphical user interface (GUI) prototypes by decomposing high-level textual descriptions into fine-grained GUI requirements. Integrated with the prototyping tool Figma, GUIDE employs a retrieval-augmented generation (RAG) technique to efficiently translate these requirements into editable Material Design components, enhancing controllability and streamlining the prototyping process. 
                    </div>
                </div>
            </div>
        </a>


        <h2>ICLR Workshops</h2>

        <a href="https://openreview.net/forum?id=u2Hh24rxW1&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FWorkshop%2FNFAM%2FAuthors%23your-submissions)" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/marton.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">Decision Trees That Remember: Gradient-Based Learning of Recurrent Decision Trees with Memory</div>
                    <div class="tags">
                        <div class="tag">University of Mannheim</div>
                        <div class="tag">Boehringer Ingelheim</div>
                        <div class="tag">TU Clausthal</div>
                        <div class="tag">University of Rostock</div>
                        
                    </div>
                    <div class="description">
                        The paper introduces ReMeDe Trees, a novel decision tree architecture designed to handle sequential data by integrating an internal memory mechanism akin to that of Recurrent Neural Networks (RNNs). Unlike traditional decision trees that rely on feature engineering to capture temporal dependencies, ReMeDe Trees learn hard, axis-aligned decision rules for both output generation and state updates, optimized efficiently via gradient descent. 
                    </div>
                </div>
            </div>
        </a>

        <a href="https://arxiv.org/abs/2501.08925" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/grams_disentangling.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">Disentangling Exploration of Large Language Models by Optimal Exploitation</div>
                    <div class="tags">
                        <div class="tag">TU Clausthal</div>
                        <div class="tag">University of Mannheim</div>
                    </div>
                    <div class="description">
                        The paper investigates the exploration capabilities of large language models (LLMs) by isolating exploration as the sole objective and introducing a framework that decomposes missing rewards into exploration and exploitation components based on the optimal achievable return.
                    </div>
                </div>
            </div>
        </a>

        <a href="https://arxiv.org/abs/2503.08738" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/zenkner_shedding.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">Shedding Light on Task Decomposition in Program Synthesis: The Driving Force of the Synthesizer Model</div>
                    <div class="tags">
                        <div class="tag">TU Clausthal</div>
                    </div>
                    <div class="description">
                        This paper compares ExeDec, a program synthesis framework that uses explicit task decomposition, with REGISM, its variant that relies solely on iterative execution-guided synthesis. 
                        While ExeDec shows strong performance in length generalization and concept composition due to its decomposition strategy, REGISM often matches or exceeds its performance, suggesting that repeated execution can be equally or more effective in many scenarios.
                    </div>
                </div>
            </div>
        </a>

        <a href="https://arxiv.org/abs/2403.07733" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/dseg_lime.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">Beyond Pixels: Enhancing LIME with Hierarchical Features and Segmentation Foundation Models</div>
                    <div class="tags">
                        <div class="tag">TU Clausthal</div>
                        <div class="tag">University of Mannheim</div>
                    </div>
                    <div class="description">
                        The paper introduces DSEG-LIME, an improved framework for Local Interpretable Model-agnostic Explanations (LIME) in image analysis. 
                        By integrating data-driven segmentation through foundation models and enabling user-controlled hierarchical segmentation, DSEG-LIME enhances interpretability by aligning explanations more closely with human-recognized concepts, outperforming traditional methods on several explainable AI metrics.
                    </div>
                </div>
            </div>
        </a>

        <a href="https://arxiv.org/abs/2503.09159" class="card-link">
            <div class="card">
                <div class="left">
                    <img src="assets/tschalzev_unreflected.png" alt="Paper Preview">
                </div>
                <div class="right">
                    <div class="title">Unreflected Use of Tabular Data Repositories Can Undermine Research Quality</div>
                    <div class="tags">
                        <div class="tag">University of Mannheim</div>
                        <div class="tag">TU Clausthal</div>
                        <div class="tag">University of Rostock</div>
                        <div class="tag">University of Freiburg</div>
                    </div>
                    <div class="description">
                        The paper highlights that indiscriminate utilization of datasets from repositories like OpenML may compromise research integrity. 
                        The authors present cases illustrating issues such as suboptimal model selection, neglect of robust baselines, and improper preprocessing, and propose enhancements to data repository practices to bolster empirical research standards. 
                    </div>
                </div>
            </div>
        </a>

    </div>
</body>
</html>